---
layout: post
title:  "Retour sur la Xebicon 2018"
description: "Xebicon 2018 du 20 novembre, ce que j'ai aime et retenu"
date:   2018-11-20 11:00:00
tags: conference
---

Le 20 novembre dernier avait lieu la Xebicon, la conférence annuel de Xebia, une SS2I parisienne bien connue. Les deux premiers rangs de toutes les salles étaient réservés "Tech4Exec", place vendu au "C-Levels". Si certain y voit un des premiers signe du rachat de Xebia par Publicis, cela les regardes ! Moi en tout cas j'ai passé la journée au 1er rang comme tout bon "C-Levels" :-)

Au programme :

* [Deno, le "peut-être" futur node.js](#deno)
* [Optimisation d'enchère adWords par Oui.sncf](#adwords)
* [MlFlow, la datascience de façon un peu plus pro](#mlflow)
* [Une vulgarisation du deepLearning distribué](#dl-distributed)
* [Zeppelin comme outil de reporting](#zeppelin)
* [sparkNLP : traitement diu langage naturel avec Spark](#spark)
* [comment tricher avec vos DataViz](#dataviz)

### Deno, le "peut-être" futur node.js <a class="anchor" id="deno"></a>

Rohald Dahl, le créateur de node.js en 2009 n'est apparemment pas très satisfait de son travail initial et de ce qu'est devenu node.js et son ecosystème.

Le projet a démarré il y a 6 mois seulement mais est apparemment très actif.

pour l'instant bien à l'arrache : lister ce que j'ai compris pas mettre en 1er.

### Optimisation d'enchère adWords par Oui.sncf <a class="anchor" id="adwords"></a>

La feature team **Astro** de chez Oui.sncf est venue nous montrer le projet OuiBoost. Un produit qui permet aux webmarketers d'optimiser le ROI de leur campagne adWords en leur permettant de choisir les meilleurs mots clés et la meilleure enchère affin de maximiser leur ROI : soit un minimum d'enchère pour un maximum de clique.

Ils utilisent l'APIs google d'adWords pour obtenir leur données d'entrainement et leurs pipeline est composé de 2 prédicteurs et d'un moteur d'optimisation.

Un premier prédicteur = prediction du cout pour un mot clé donné
Un second predisant le revenu à prévoir pour un mot clé donné
J'imagine que le jeu de données sortie de ces deux prédicteurs sert de matière à l'algorithme de type glouton qui leur permet de choisir sur quels mots clés investir.

Ils ne sont pas entré profondément dans la technique hélas mais l'idée reste très intéressante.

### MlFlow, la datascience de façon un peu plus pro <a class="anchor" id="mlflow"></a>

3 modules dans ce framework open source pour bosser plus proprement en datascience :

* Tracking :

Garder une trace des paramètres d'un modéle entrainé et de ces résultats

* Projects :

Packaging d'une expérimentation pour la rendre reproduisible : data d'entrainement de validation, model et paramètres dans un fichier yml

* Models :

Build et pousse un livrable permettant de pousser en production un modèle, par exemple un fichier pkl à destination d'une webapp en prod permettant de faire de prédiction avec la version n - 1.

Très prometteur, je m'en vais approfondir ça de ce pas !

### Une vulgarisation du deepLearning distribué <a class="anchor" id="dl-distributed"></a>

Une petite prez d'un quart d'heure pour expliquer comment se passe le ml distribué pour pouvoir entrainer des réseaux de neuronnes très profond avec des millions de données d'entrainement.

Duplicate et synchronize VS Split le réseaux
Le choix en fonction du type de réseau et des calculs qu'il implique et de la volumétrie des données que l'on a.

Horovod : Uber distributed framework
Keras : Distributed Keras ou Elephas = librairie pour faire du distribué

### Zeppelin comme outil de reporting <a class="anchor" id="zeppelin"></a>

### sparkNLP : traitement du langage naturel avec Spark <a class="anchor" id="spark"></a>

Je ne connais que les grandes lignes de Spark et je n'ai jamais eu l'occasion de jouer avec. Dans cette conf, @auroredea nous présente la trés récente API spark-NLP qui vient juste au dessus de spark-ML.

J'imagine qu'avec la puissance d'un cluster Spark, on pourrait entrainer des models permettant de faire de la NER assez rapidement et sur de gros volumes de données (dont il faut déjà être assez chanceux pour les avoir à disposition).

Par contre, en apprennant dans ce talk que les models pré-entrainé de spark-ML ne sont dispo que pour l'anglais et qu'on ne peut entrainer des models que sur des jeux de moins de 2Mo, je me dis qu'il reste encore de la route pour rivaliser avec des librairies comme Spacy et qui ont le bon goût d'etre en python :-)

### comment tricher avec vos DataViz <a class="anchor" id="dataviz"></a>

Sous un titre volontairement provocateur @Romain Sagean nous montre quelques ficelles pour tricher dans nos graphiques.

Il nous montre comment on peut diminuer l'effet visuel d'une baisse dans un line chart en augmentant l'épaisseur du trait ou même en le passant en 3D.

En manipulant un peu les échelles, on peut facilement augmenter l'effet visuel d'une augmentation ou diminuer son effet avec des bars charts.

De manière général, toujours utiliser les line chart pour montrer une évolution dans le temps et les bar charts pour faire de la comparaison.

Concernant les fameux camembert, autrement appeler pie chart, les découper en leurs centres pour leur donner un effet Donuts TM est une mauvaise idée meme si ca donne un vrai style et une patte de designer. En effet, selon Romain, 25% des gens regardent l'angle du camembert pour visualiser la part d'une catégorie quand les autres regardent la périphérie.

La 3D est toujours à prescrire, en mettant volontairement une catégorie sous représentées en haut du camembert, on augmente l'impression de son importance artificiellement.

Les couleurs ont leurs importance, un bar chart qui compare des pays avec la France en rose et le Brésil en violet, c'est du grand n'importe quoi.

Et pour finir quelque chose de vraiment basique. Quand on présente un tableau tout bête, le trier sur les labels (ex: le nom des pays) est souvent une mauvaise idée. Il vaut mieux trier sur le chiffre d'affaire du pays dans l'ordre que l'on veut par exemple.
